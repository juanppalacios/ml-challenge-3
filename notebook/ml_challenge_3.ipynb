{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIS 678 - Machine Learning - Kaggle Challenge 3: MNIST Digit Classification\n",
    "\n",
    "<!-- ## Data Loading -->\n",
    "\n",
    "<!-- ## Model Implementation -->\n",
    "\n",
    "<!-- ## Model Evaluation -->\n",
    "\n",
    "<!-- ## Conclusion -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Configuration\n",
    "Before running our model, let's change our working directory over to our Python scripts by running the following shell script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!echo sourcing MNIST model directory...\n",
    "%cd '../model/src/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing our Custom Neural Network Libraries\n",
    "Now that were are under the right directory, let's start off by importing all of our custom Python scripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# custom modules\n",
    "from toolkit import Toolkit\n",
    "from parameters import ParameterManager\n",
    "from model import Model\n",
    "\n",
    "# creating our notebook toolkit helper\n",
    "tools = Toolkit()\n",
    "tools.configure(name = 'Notebook', level = 'DEBUG')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in Data\n",
    "This model will train on the MNIST data set.\n",
    "A collection of three `csv` files are imported to generate our training and testing data sets.\n",
    "Let's load in our `mnist_train.csv`, `mnist_train_targets.csv`, and `mnist_test.csv` files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools.info(\"loading in our MNIST data sets\")\n",
    "\n",
    "# train_data    = tools.load_data(\"../data/train/mnist_train.csv\")\n",
    "# train_targets = tools.load_data(\"../data/train/mnist_train_targets.csv\", transpose = False)\n",
    "# test_data     = tools.load_data(\"../data/test/mnist_test.csv\")\n",
    "# test_targets  = np.zeros((test_data.shape[0], 1), dtype=int)\n",
    "\n",
    "tools.info(\"applying normalization to our train data\")\n",
    "\n",
    "# train_data = tools.normalize(train_data)\n",
    "# test_data  = tools.normalize(test_data)\n",
    "\n",
    "tools.info(\"visualizing a few MNIST training samples...\")\n",
    "# tools.visualize(train_data)\n",
    "\n",
    "# tools.info(\"visualizing a few MNIST testing samples...\")\n",
    "# tools.visualize(test_data)\n",
    "\n",
    "tools.warning(\"plots should be labaled!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter Configuration\n",
    "Parameter selection is model development makes it easier to optimize our model.\n",
    "For example, if we can tune our model's learning rate value and see its effects on model performance.\n",
    "Using our custom `ParameterManager` class, we can easily create a variety of parameters to pass onto our MNIST model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper-parameter manager\n",
    "parameters = ParameterManager()\n",
    "\n",
    "# create learning parameters\n",
    "parameters.add_parameter(epochs = [5])\n",
    "parameters.add_parameter(learning_rate = [0.001])\n",
    "\n",
    "# create an architecture parameter\n",
    "parameters.add_nested_parameter(hidden_layers = [2], hidden_dimensions = [64, 32, 16], activation = ['relu'])\n",
    "\n",
    "tools.debug(f'showing notebook parameters: {parameters}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST Model Configuration and Compilation\n",
    "With our training and testing data sets ready for processing and all of our tunable parameters declared, we can start compiling our custom neural architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_model = Model(debug_mode = True)\n",
    "\n",
    "mnist_model.configure(parameters = parameters)\n",
    "\n",
    "# mnist_model.summary()\n",
    "\n",
    "# mnist_model.add_input_layer(784, 784, 'none')\n",
    "# mnist_model.summary()\n",
    "# mnist_model.add_hidden_layer(784, 64, 'relu')\n",
    "# mnist_model.summary()\n",
    "# mnist_model.add_hidden_layer(64, 32, 'sigmoid')\n",
    "# mnist_model.summary()\n",
    "# mnist_model.add_output_layer(32, 10, 'softmax')\n",
    "# mnist_model.summary()\n",
    "\n",
    "mnist_model.fit()\n",
    "# mnist_model.fit(train_data, train_targets)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assessing Model Performance\n",
    "The following section describes our model performance over all parameter variations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scores = mnist_model.evaluate(test_data, test_targets)\n",
    "\n",
    "# tools.info(\"saving our test targets\")\n",
    "\n",
    "# tools.save_data(\"../data/test/mnist_test_targets.csv\", test_targets)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
